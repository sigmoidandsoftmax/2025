<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let t=t=>t.trim(),e=t=>t.innerText,n=t=>{let e=t.split(" "),n=e.slice(0,-1).join(" ");return[e.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(e).map(t).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(t=>"P"===t.nodeName).map(e).map(t),"April 28, 2025"),r="Sigmoid and Softmax",l="Explanations of sigmoid and softmax rely on probabilistic interpretation of the output(s) but do not provide the source of probabilistic interpretation. In contrast, both sigmoid and softmax have a simple definition with their roots in basic information theory and calculus. This blog takes an isolated look at sigmoid and softmax and explains how and why they materialize. The definition and derivation show that they are related.";{let t=a.map(t=>`${t[0]}, ${t[1]}`).join(" and "),e=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${t}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {ICLR Blogposts 2025},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=e}{let t=a.map(t=>t[0]),e=`\n${t=t.length>2?t[0]+", et al.":2==t.length?t[0]+" & "+t[1]:t[0]}, "${r}", ICLR Blogposts, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=e}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Sigmoid and Softmax | ICLR Blogposts 2025</title> <meta name="author" content="ICLR Blog"> <meta name="description" content="Explanations of sigmoid and softmax rely on probabilistic interpretation of the output(s) but do not provide the source of probabilistic interpretation. In contrast, both sigmoid and softmax have a simple definition with their roots in basic information theory and calculus. This blog takes an isolated look at sigmoid and softmax and explains how and why they materialize. The definition and derivation show that they are related."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning, iclr"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/2025/assets/img/iclr_favicon.ico"> <link rel="stylesheet" href="/2025/assets/css/main.css"> <link rel="canonical" href="https://iclr-blogposts.github.io/2025/blog/sigmoid-and-softmax/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/2025/assets/js/theme.js"></script> <script src="/2025/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/2025/assets/js/distillpub/template.v2.js"></script> <script src="/2025/assets/js/distillpub/transforms.v2.js"></script> <script src="/2025/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px;\}</style> <d-front-matter> <script async type="text/json">{
      "title": "Sigmoid and Softmax",
      "description": "Explanations of sigmoid and softmax rely on probabilistic interpretation of the output(s) but do not provide the source of probabilistic interpretation. In contrast, both sigmoid and softmax have a simple definition with their roots in basic information theory and calculus. This blog takes an isolated look at sigmoid and softmax and explains how and why they materialize. The definition and derivation show that they are related.",
      "published": "April 28, 2025",
      "authors": [
        {
          "author": "Anonymous",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/2025/">ICLR Blogposts 2025</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/2025/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/call/">call for blogposts</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/reviewing/">reviewing</a> </li> <li class="nav-item "> <a class="nav-link" href="/2025/blog/index.html">blog</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">past iterations</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2025/"><strong>2025</strong></a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2024/">2024</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blogposts.github.io/2023/">2023</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="https://iclr-blog-track.github.io/home/" rel="external nofollow noopener noopener noreferrer" target="_blank">2022</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Sigmoid and Softmax</h1> <p>Explanations of sigmoid and softmax rely on probabilistic interpretation of the output(s) but do not provide the source of probabilistic interpretation. In contrast, both sigmoid and softmax have a simple definition with their roots in basic information theory and calculus. This blog takes an isolated look at sigmoid and softmax and explains how and why they materialize. The definition and derivation show that they are related.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#sigmoid">Sigmoid</a></div> <div><a href="#softmax">Softmax</a></div> <div><a href="#discussion-and-conclusion">Discussion and Conclusion</a></div> <div><a href="#references">References</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Let \(x \in \mathbf{R}\). Sigmoid function \(\sigma : \mathbf{R} \rightarrow \left[0, 1\right]\) is given by \(\begin{align} \sigma\left(x\right) &amp;= \frac{1}{1 + \exp\left(-x\right)}. \end{align}\) Sigmoid function is used in logistic regression and binary classification.</p> <p>Let \(\mathbf{x} \in \mathbf{R^d}\) and \(\mathbf{S} = \left\{\left(\mathbf{w}_0, \mathbf{w}_1, \ldots, \mathbf{w}_d\right) \mid \sum_{i=1}^d \mathbf{w}_i = 1\right\} \subset \left[0, 1\right]^d\). Softmax function \(\mathbf{f} : \mathbf{R^d} \rightarrow \mathbf{S}\) is given by \(\begin{align} \mathbf{f}\left(\mathbf{x}\right)_i &amp;= \frac{\exp{\left(-\mathbf{x}_i\right)}}{\sum_{j=1}^d \exp{\left(-\mathbf{x}_j\right)}}. \end{align}\)</p> <p>Softmax function is used in multiclass classification, smooth maximum and softargmax. Softmax does not approximate the \(\arg\max\) function but approximates the one-hot encoding of the \(\arg\max\) function [1]. Let \(\mathbf{p} \in \left\{0, 1\right\}^d\), \(\mathbf{\hat{p}} \in \left[0, 1\right]^d\) and \(\mathbf{p}, \mathbf{\hat{p}} \in \mathbf{S}\). Then \(\begin{align} \left(\mathbf{p}_0, \mathbf{p}_1, \ldots, \mathbf{p}_i, \ldots, \mathbf{p}_d\right) &amp;\approx \left(\mathbf{\hat{p}}_0, \mathbf{\hat{p}}_1, \ldots, \mathbf{\hat{p}}_i, \ldots, \mathbf{\hat{p}}_d\right) \\ &amp;= \mathbf{f\left(\mathbf{x}\right)}. \end{align}\)</p> <p>The approximation error is quantified by cross entropy \(\begin{align} \mathcal{L} &amp;= \sum_{i=1}^d -\mathbf{p}_i\log\left(\mathbf{\hat{p}}_i\right). \end{align}\)</p> <p>This should not confuse the reader whether softmax does or does not have its roots in $\arg\max$. The next section shows that indeed it does not. But it is still not known where softmax arises from. An analysis of an online time-series algorithm sheds some light on the softmax function [2] and the derivation of softmax can be established. Derivation of sigmoid is discussed before proceeding to softmax.</p> <h2 id="sigmoid">Sigmoid</h2> <p>Consider the transformation \(\begin{align} p &amp;= \arg\min_{ w \in [0,1]} wx \end{align}\)</p> <p>\(p\) with entropy regularizer \(\begin{align} \hat{p} &amp;= \arg\min_{ w \in \left[0,1\right] } wx - w\log\left(w\right) - \left(1-w\right)\log\left(1-w\right) \end{align}\)</p> <p>Differentiate the objective function with respect to $w$ and equate to 0 \(\begin{align} x - 1 - \log(w) + 1 + \log(1-w) &amp;= 0 \\ \log(1-w) - \log(w) &amp;= -x \\ \log\left(\frac{1-w}{w}\right) &amp;= -x \\ \log\left(\frac{1}{w} - 1\right) &amp;= -x \\ \frac{1}{w} &amp;= 1 + \exp(-x) \\ w^\star &amp;= \frac{1}{1+\exp(-x)} \\ \hat{p} &amp;= w^\star \end{align}\)</p> <h2 id="softmax">Softmax</h2> <p>Consider the transformation \(\begin{align} \mathbf{p} &amp;= \arg\min_{ \mathbf{w} \in \mathbf{S} } \langle \mathbf{w}, \mathbf{x} \rangle \end{align}\) where \(\mathbf{x} \in \mathbf{R^d}, \mathbf{w} \in \mathbf{S} \subset \left[0, 1\right]^d\) such that \(\mathbf{S} = \left\{\mathbf{w} \mid \sum_{i}^d \mathbf{w}_i = 1\right\}\), \(\mathbf{p} \in \left[0, 1\right]^d\) and \(\sum_{i=1}^d \mathbf{p}_i = 1\).</p> <p>\(\mathbf{p}\) with negative entropy regularizer \(\begin{align} \mathbf{\hat{p}} &amp;= \arg\min_{ \mathbf{w} \in \mathbf{S} } \langle \mathbf{w}, \mathbf{x} \rangle + \sum_{i=1}^d \mathbf{w}_i\log\left(\mathbf{w}_i\right) \end{align}\) Since \(\mathbf{w} \in \mathbf{S}\), add a Lagrange multiplier \(\lambda\left(\langle \mathbf{w}, \mathbf{1} \rangle - 1\right)\) to the objective function. \(\begin{align} \mathbf{\hat{p}} &amp;= \arg\min_{ \mathbf{w} \in \mathbf{S} } \langle \mathbf{w}, \mathbf{x} \rangle + \sum_{i=1}^d \mathbf{w}_i\log\left(\mathbf{w}_i\right) + \lambda\left(\langle \mathbf{w}, \mathbf{1} \rangle - 1\right) \\ &amp;= \arg\min_{ \mathbf{w} \in \mathbf{S} } \sum_{i=1}^d \mathbf{w}_i \mathbf{x}_i + \sum_{i=1}^d \mathbf{w}_i\log\left(\mathbf{w}_i\right) + \lambda\left(\sum_{i=1} ^d \mathbf{w}_i - 1\right) \end{align}\) Differentiate the objective function with respect to \(\mathbf{w}_i\) and equate to 0 \(\begin{align} \mathbf{x}_i + 1 + \log\left(\mathbf{w}_i\right) + \lambda &amp;= 0 \\ \mathbf{w}_i^\star &amp;= \exp\left(-\mathbf{x}_i\right)\exp\left(-1 - \lambda\right) \\ &amp;= \frac{\exp\left(-\mathbf{x}_i\right)}{\exp\left(1 + \lambda\right)} \end{align}\) Set \(\lambda\) such that \(\sum_{i}^d \mathbf{w}_i^\star = 1\) \(\begin{align} \mathbf{w}_i^\star &amp;= \frac{\exp\left(-\mathbf{x}_i\right)}{\sum_{j=1}^d\exp\left(-\mathbf{x}_j\right)} \\ \mathbf{\hat{p}} &amp;= \mathbf{w}^\star \end{align}\)</p> <p>Note that $\langle \mathbf{w}^\star, \mathbf{x} \rangle$ is smooth maximum whereas \(\langle \mathbf{w}, \begin{bmatrix} 1&amp; 2&amp; \ldots &amp; d \end{bmatrix} \rangle\) for some $\mathbf{w} \in \mathbf{S}$ is softargmax [3].</p> <h2 id="discussion-and-conclusion">Discussion and Conclusion</h2> <p>Figure 1 shows the values of the objective function for different weights to the entropy term.</p> <p><a target="_blank" href="https://github.com/sigmoidandsoftmax/2025/blob/main/assets/img/2025-04-28-sigmoid-and-softmax/sigmoid_neg_entropy.png?raw=true" rel="external nofollow noopener noopener noreferrer"><img src="https://github.com/sigmoidandsoftmax/2025/blob/main/assets/img/2025-04-28-sigmoid-and-softmax/sigmoid_neg_entropy.png?raw=true" alt="Sigmoid with entropy" width="500"></a></p> <p><a target="_blank" href="https://github.com/sigmoidandsoftmax/2025/blob/main/assets/img/2025-04-28-sigmoid-and-softmax/softmax_neg_entropy.png?raw=true" rel="external nofollow noopener noopener noreferrer"><img src="https://github.com/sigmoidandsoftmax/2025/blob/main/assets/img/2025-04-28-sigmoid-and-softmax/softmax_neg_entropy.png?raw=true" alt="Softmax with entropy" width="500"></a></p> <p>Figure 1. Effect of the entropy term in values of the objective function corresponding to sigmoid and softmax.</p> <p>Figure 1 (top) shows the effect of entropy term in value of the objective function corresponding to sigmoid. The curve obtained by fixing the output probability and varying only the weight to the entropy term has a sigmoidal shape. On the other hand the curve obtained by fixing the weight to the entropy term and varying the output probability has a parabolic shape. Sigmoid is the function when applied to a scalar gives the probability that has minimum entropy and multiplication value with the input scalar.</p> <p>Figure 1 (bottom) shows the effect of entropy term in value of the objective function corresponding to softmax. The curve obtained by fixing the output probabilities and varying only the weight to the entropy term has a sigmoidal shape. On the other hand the region obtained by fixing the weight to the entropy term and upper bounding value of the objective function and varying the output probabilities is an ellipse. Softmax is the function that when applied on a vector gives a probability vector that has minimum negative entropy and dot product value with the input vector.</p> <h2 id="references">References</h2> <ol> <li>Ian Goodfellow, Yoshua Bengio and Aaron Courville. Deep Learning. 2016.</li> <li><a href="https://lucatrevisan.github.io/40391/lecture12.pdf" rel="external nofollow noopener noopener noreferrer" target="_blank">Luca Trevison. The ``Follow-the-Regularized-Leader’’ algorithm. Topics in computer science and optimization (Fall 2019).</a></li> <li>Ross Goroshin, Michael Mathieu and Yann LeCun. Learning to Linearize Under Uncertainty. In Advances in Neural Information Processing Systems, 2015.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/2025/assets/bibliography/2025-04-28-sigmoid-and-softmax.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>